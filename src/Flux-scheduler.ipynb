{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db667b-8a5b-47fb-a595-98140fb613b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flux_lora import TextImageDataset\n",
    "from diffusers import FluxPipeline, FlowMatchEulerDiscreteScheduler\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def decode_and_display_latent(cur_latent, prev_latent, noise, vae):\n",
    "    # Unpack latents using FLUX pipeline's method\n",
    "    batch_size, num_patches, channels = cur_latent.shape\n",
    "    \n",
    "    # VAE applies 8x compression on images but we must also account for packing which requires\n",
    "    # latent height and width to be divisible by 2\n",
    "    height = int(np.sqrt(num_patches)) * 2  # Multiply by 2 since we packed 2x2 patches\n",
    "    width = height\n",
    "    \n",
    "    # Unpack current latents\n",
    "    cur_latent = cur_latent.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n",
    "    cur_latent = cur_latent.permute(0, 3, 1, 4, 2, 5)\n",
    "    cur_latent = cur_latent.reshape(batch_size, channels // 4, height, width)\n",
    "    \n",
    "    # Unpack previous latents\n",
    "    prev_latent = prev_latent.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n",
    "    prev_latent = prev_latent.permute(0, 3, 1, 4, 2, 5)\n",
    "    prev_latent = prev_latent.reshape(batch_size, channels // 4, height, width)\n",
    "\n",
    "    # Unpack noise the same way\n",
    "    noise = noise.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n",
    "    noise = noise.permute(0, 3, 1, 4, 2, 5)\n",
    "    noise = noise.reshape(batch_size, channels // 4, height, width)\n",
    "    \n",
    "    # Scale the latents back to VAE range\n",
    "    cur_latent_scaled = (cur_latent / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "    prev_latent_scaled = (prev_latent / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "    \n",
    "    # Decode latents to images\n",
    "    with torch.no_grad():\n",
    "        cur_image = vae.decode(cur_latent_scaled).sample\n",
    "        prev_image = vae.decode(prev_latent_scaled).sample\n",
    "    \n",
    "    # Convert to display format\n",
    "    cur_image = (cur_image / 2 + 0.5).clamp(0, 1)\n",
    "    prev_image = (prev_image / 2 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    # Display images\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    ax1.imshow(cur_image[0].permute(1, 2, 0).cpu().numpy())\n",
    "    ax1.set_title('Current Timestep')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(prev_image[0].permute(1, 2, 0).cpu().numpy())\n",
    "    ax2.set_title('Previous Timestep')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Display noise - take mean across channels for visualization\n",
    "    noise_viz = noise[0].mean(dim=0).cpu().numpy()\n",
    "    noise_viz = (noise_viz - noise_viz.min()) / (noise_viz.max() - noise_viz.min())\n",
    "    ax3.imshow(noise_viz, cmap='viridis')\n",
    "    ax3.set_title('Noise (Channel Mean)')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# def test_noise_process():\n",
    "# Initialize pipeline components\n",
    "pipeline = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Get components needed for testing\n",
    "vae = pipeline.vae\n",
    "tokenizer = pipeline.tokenizer\n",
    "tokenizer_2 = pipeline.tokenizer_2\n",
    "noise_scheduler = pipeline.scheduler\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Move models to device and set to eval mode\n",
    "vae = vae.to(device)\n",
    "vae.eval()\n",
    "\n",
    "# Create transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(768, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(768),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Create dataset and get one image\n",
    "dataset = TextImageDataset(\n",
    "    image_dir=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/training_images\",\n",
    "    prompt_file=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/prompts.json\",\n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_2=tokenizer_2,\n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "# Get first image\n",
    "batch = dataset[0]\n",
    "image = batch[\"images\"].unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Encode image to latent space\n",
    "with torch.no_grad():\n",
    "    latents = vae.encode(image).latent_dist.sample() * 0.18215\n",
    "\n",
    "# Pack latents\n",
    "batch_size, num_channels, height, width = latents.shape\n",
    "latents = pipeline._pack_latents(latents, batch_size, num_channels, height, width)\n",
    "\n",
    "# Sample timesteps at different points in the diffusion process\n",
    "timesteps = [103, 455, 781]  # Beginning, middle, and end of diffusion\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "noise = torch.randn_like(latents)\n",
    "for t in timesteps:\n",
    "    print(f\"\\nVisualizing noise at timestep {t}\")\n",
    "    timestep = torch.tensor([t], device=device).long()\n",
    "    prev_timestep = torch.tensor([t-100], device=device).long()\n",
    "    \n",
    "    # Generate noisey    \n",
    "    \n",
    "    # Get noised latents at both timesteps\n",
    "    noisy_latents = noise_scheduler.scale_noise(\n",
    "        sample=latents,\n",
    "        timestep=timestep,\n",
    "        noise=noise\n",
    "    )\n",
    "    \n",
    "    prev_noisy_latents = noise_scheduler.scale_noise(\n",
    "        sample=latents,\n",
    "        timestep=prev_timestep,\n",
    "        noise=noise\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    decode_and_display_latent(noisy_latents, prev_noisy_latents, noise, vae)\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image[0].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c2f606-e4f9-4f3f-b9aa-5ff9cc140f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(25)\n",
    "scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e36e27-23fb-4796-81ce-a021d5edf37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(25)\n",
    "scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782e16d-71f6-4798-b8b5-55f6d0547c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = noise\n",
    "for (i,t) in enumerate(scheduler.timesteps):\n",
    "    with torch.no_grad():\n",
    "        noisy_residual = model(input_vector, t).sample\n",
    "    previous_noisy_sample = scheduler.step(noisy_residual, t, input_vector).prev_sample\n",
    "    input_vector = previous_noisy_sample\n",
    "    if i%5==0:\n",
    "        print(i)\n",
    "        display_inputvector(input_vector)\n",
    "print(i)\n",
    "display_inputvector(input_vector);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc1088-5337-49b7-82d9-5a3855b716e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcda33-f34f-4e75-886a-f81a1c587f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_inputvector(input_vector)\n",
    "def display_inputvector(input_vector):\n",
    "    image = (input_vector / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "    image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n",
    "    image = Image.fromarray(image)\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4fe2b9-3393-430f-bf22-d569b943322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FluxPipeline, FlowMatchEulerDiscreteScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7845920-707d-4596-9f98-78db8ea15ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmed_scheduler = FlowMatchEulerDiscreteScheduler(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5892a-4482-442f-9d08-a595f500fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmed_scheduler.timesteps;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b40943-5881-4f1d-91bb-e006b08e653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmed_scheduler.scale_noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f798a6-ebe3-4b10-9495-d3e5ba63eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines import FluxPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9199b39-243d-4405-811e-931bf594e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b087a-4ab0-4265-bdfd-0687056d2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.transformer.config.guidance_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce44fdb-0549-44dc-b2b4-2545d2ef9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n",
    "guidance = guidance.expand(latents.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98602fc9-4ce5-40da-a440-e76f9701b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FluxTransformer2DModel.config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863908b8-1027-4215-9e0e-2db762b9cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ce83f-8514-49cc-ba25-bf23830ca34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sample_size = model.config.sample_size\n",
    "noise = torch.randn((1, 3, sample_size, sample_size), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfcecd-abbc-4a9c-b276-1f60f480c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sample_size: {sample_size}\")\n",
    "print(f\"noise shape: {noise.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab4e577-4274-4259-ab2f-3a41664dd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = noise\n",
    "\n",
    "for t in scheduler.timesteps:\n",
    "    with torch.no_grad():\n",
    "        noisy_residual = model(input, t).sample\n",
    "    # previous_noisy_sample = scheduler.step(noisy_residual, t, input_vector).prev_sample\n",
    "    # input_vector = previous_noisy_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b30239-d38c-4f62-b541-581eb8950864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
