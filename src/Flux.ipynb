{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7330f455-264a-4e12-843c-8d833281baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: diffusers in /home/ubuntu/.local/lib/python3.10/site-packages (0.33.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from diffusers) (2.25.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from diffusers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from diffusers) (4.6.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from diffusers) (9.0.1)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from diffusers) (1.21.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from diffusers) (0.30.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.27.0->diffusers) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.27.0->diffusers) (5.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.27.0->diffusers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.27.0->diffusers) (21.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.51.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/lib/python3/dist-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/lib/python3/dist-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in /home/ubuntu/.local/lib/python3.10/site-packages (0.15.1)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (4.51.2)\n",
      "Requirement already satisfied: safetensors in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from peft) (21.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from peft) (1.21.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from peft) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/lib/python3/dist-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.25.0->peft) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.25.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub>=0.25.0->peft) (3.6.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface_hub>=0.25.0->peft) (2.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers->peft) (0.21.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install diffusers\n",
    "! pip install transformers\n",
    "! pip install accelerate\n",
    "! pip install sentencepiece\n",
    "! pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8fe6f82-2c78-4d26-8263-829e9396025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:00<00:00, 29.24it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 44.95it/s]\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:00<00:00, 14.12it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 57.45it/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 15.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77174c0-5fad-4fc0-92d0-6f8f328982e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('patch_size', 1),\n",
       "            ('in_channels', 64),\n",
       "            ('out_channels', None),\n",
       "            ('num_layers', 19),\n",
       "            ('num_single_layers', 38),\n",
       "            ('attention_head_dim', 128),\n",
       "            ('num_attention_heads', 24),\n",
       "            ('joint_attention_dim', 4096),\n",
       "            ('pooled_projection_dim', 768),\n",
       "            ('guidance_embeds', True),\n",
       "            ('axes_dims_rope', (16, 56, 56)),\n",
       "            ('_use_default_values', ['axes_dims_rope', 'out_channels']),\n",
       "            ('_class_name', 'FluxTransformer2DModel'),\n",
       "            ('_diffusers_version', '0.30.0.dev0'),\n",
       "            ('_name_or_path',\n",
       "             '/home/ubuntu/.cache/huggingface/hub/models--black-forest-labs--FLUX.1-dev/snapshots/0ef5fff789c832c5c7f4e127f94c8b54bbcced44/transformer')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.transformer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef567e8-3196-4d78-9323-55b3a205d1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6bfea-a77b-452e-9936-ac66ce3c9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-dev.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689bce7-977a-4261-9778-3c510cc87989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2f457-56c1-4005-9af4-44cfc05a2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEPS=[1,2,3,4,5,10,20]\n",
    "# for i in STEPS:\n",
    "#     prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=i,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"0-flux-dev-{i}.png\")\n",
    "\n",
    "\n",
    "# for i in STEPS:\n",
    "#     prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=i,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(1)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"1-flux-dev-{i}.png\")\n",
    "\n",
    "\n",
    "# for i in STEPS:\n",
    "#     prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=i,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"0-flux-dev-{i}.png\")\n",
    "\n",
    "\n",
    "# for i in [30,50]:\n",
    "#     prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=i,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(1)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"1-flux-dev-{i}.png\")\n",
    "\n",
    "# for i in [30,50]:\n",
    "#     prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=i,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"0-flux-dev-{i}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee347dcb-ad8f-4701-8373-d68de168b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [30,50]:\n",
    "#     prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=i,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(1)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"flux_initial_test/1-flux-dev-{i}.png\")\n",
    "\n",
    "# for i in [30,50]:\n",
    "#     prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=i,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"flux_initial_test/0-flux-dev-{i}.png\")\n",
    "\n",
    "# for i in range(2,10):\n",
    "#     prompt = \"Paul Mescal\"\n",
    "#     image = pipe(\n",
    "#         prompt,\n",
    "#         height=1024,\n",
    "#         width=1024,\n",
    "#         guidance_scale=3.5,\n",
    "#         num_inference_steps=20,\n",
    "#         max_sequence_length=512,\n",
    "#         generator=torch.Generator(\"cpu\").manual_seed(i)\n",
    "#     ).images[0]\n",
    "#     image.save(f\"flux_initial_test/{i}-flux-dev-{20}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d8033-b14d-4c14-ac65-67d42fd2265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877d0ba-e777-4c9e-b4f8-0fffdaf1fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from accelerate import Accelerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db910c1a-59ad-4ef7-a32e-1866517c8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers\n",
    "CLIP_tokenizer = pipe.tokenizer\n",
    "t5_tokenizer = pipe.tokenizer_2 \n",
    "\n",
    "CLIP_encoder = pipe.text_encoder_2\n",
    "t5_encoder = pipe.text_encoder\n",
    "\n",
    "# diffusers\n",
    "flux = pipe.transformer \n",
    "vae = pipe.vae \n",
    "scheduler = pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91259bb1-74c2-4fec-9fc7-a843b9eb5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(pipeline):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bc311-a398-441f-95e1-efddea5391dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward input\n",
    "prompt = \"Paul Mescal doing an activity\"\n",
    "prompt_2 = None\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_images_per_prompt = 1\n",
    "prompt_embeds = None\n",
    "pooled_prompt_embeds = None\n",
    "max_sequence_length: int = 512\n",
    "lora_scale = None\n",
    "latents = None\n",
    "sigmas = None\n",
    "\n",
    "#inference output\n",
    "height=1024\n",
    "width=1024\n",
    "guidance_scale=3.5\n",
    "num_inference_steps=20\n",
    "max_sequence_length=512\n",
    "generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "\n",
    "#training\n",
    "batch_size=4\n",
    "\n",
    "# prompt_embeds: torch.Size([1, 512, 4096])\n",
    "# pooled_prompt_embeds: torch.Size([1, 768])\n",
    "# text_ids: torch.Size([512, 3])\n",
    "(prompt_embeds, pooled_prompt_embeds, text_ids)  = pipe.encode_prompt(prompt=prompt,\n",
    "                   prompt_2=prompt_2,\n",
    "                   prompt_embeds=prompt_embeds,\n",
    "                   pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                   device=device,\n",
    "                   num_images_per_prompt=num_images_per_prompt,\n",
    "                   max_sequence_length=max_sequence_length,\n",
    "                   lora_scale=lora_scale,\n",
    "                  )\n",
    "\n",
    "num_channels_latents = pipe.transformer.config.in_channels // 4\n",
    "\n",
    "\n",
    "#latents: torch.Size([4, 4096, 64])\n",
    "# latent_image_ids: torch.Size([4096, 3])\n",
    "latents, latent_image_ids = pipe.prepare_latents(\n",
    "    batch_size * num_images_per_prompt,\n",
    "    num_channels_latents,\n",
    "    height,\n",
    "    width,\n",
    "    prompt_embeds.dtype,\n",
    "    device,\n",
    "    generator,\n",
    "    latents,\n",
    ")\n",
    "\n",
    "# 5. Prepare timesteps\n",
    "from diffusers.pipelines.flux.pipeline_flux import calculate_shift, retrieve_timesteps\n",
    "\n",
    "# sigmas: (20, 1)\n",
    "sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n",
    "# image_seq_len: 4096\n",
    "image_seq_len = latents.shape[1]\n",
    "\n",
    "#1.15, gets constant\n",
    "mu = calculate_shift(\n",
    "    image_seq_len,\n",
    "    pipe.scheduler.config.get(\"base_image_seq_len\", 256),\n",
    "    pipe.scheduler.config.get(\"max_image_seq_len\", 4096),\n",
    "    pipe.scheduler.config.get(\"base_shift\", 0.5),\n",
    "    pipe.scheduler.config.get(\"max_shift\", 1.15),\n",
    ")\n",
    "\n",
    "\n",
    "# The timesteps parameter specifically:\n",
    "# Determines the number of steps in the diffusion process\n",
    "# Controls the discretization of the continuous ODE (Ordinary Differential Equation) that defines how images transform from noise to clear images\n",
    "# The timesteps essentially define how finely the time dimension is discretized when solving this equation.\n",
    "\n",
    "\n",
    "# timesteps: torch.Size([num_inference_steps])\n",
    "# [1000., 983.6,... 142.5])\n",
    "timesteps, num_inference_steps = retrieve_timesteps(\n",
    "    pipe.scheduler,\n",
    "    num_inference_steps,\n",
    "    device,\n",
    "    sigmas=sigmas,\n",
    "    mu=mu,\n",
    ")\n",
    "\n",
    "num_warmup_steps = max(len(timesteps) - num_inference_steps * pipe.scheduler.order, 0)\n",
    "# self._num_timesteps = len(timesteps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319fc32-c0a9-4e0f-afa4-5cc32113fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latents = pipe(\n",
    "    prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=2,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(1),\n",
    "    output_type=\"latent\"\n",
    ")\n",
    "\n",
    "# latents: torch.Size([1, 4096, 64])\n",
    "latents.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c380d72-fb63-4417-b2ca-815eb4b6640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdfd4c-3bff-4ab7-9a11-45625ad33aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bd6b9-93bb-4b4b-a040-c137dff4726c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb1c29-0968-49a4-b165-d660b2d07a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "\n",
    "#accelerate\n",
    "\n",
    "#loss\n",
    "\n",
    "#backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf54a5-90a6-4851-9937-88d7f2e8e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params(model):\n",
    "    \"\"\"\n",
    "    Freezes all parameters in a PyTorch model except for LoRA parameters.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model whose parameters need to be selectively frozen\n",
    "        \n",
    "    Returns:\n",
    "        None, modifies the model in-place\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check if the parameter name contains \"lora\" (case-insensitive)\n",
    "        if \"lora\" not in name.lower():\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    # Optional: Print statistics about frozen vs trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(f\"Frozen parameters: {total_params - trainable_params:,} ({100 * (total_params - trainable_params) / total_params:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30db509-330e-44d9-86f5-c82dbb28c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660aaf1-61b2-4bef-94ec-f5d70bf58b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\n",
    "pipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
    "print(\"done loading huggingface KQV flux\")\n",
    "\n",
    "def get_flux_peft_model_to_train(flux, \n",
    "                        r=16,\n",
    "                        lora_alpha=32, \n",
    "                        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"proj_out\", \"proj_mlp\", \"linear\", \"linear_1\", \"linear_2\"],\n",
    "                        lora_dropout=0.05, \n",
    "                        bias=\"none\"\n",
    "    ):\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "     r=16,\n",
    "     lora_alpha=lora_alpha,\n",
    "     target_modules=target_modules,\n",
    "     lora_dropout=lora_dropout,\n",
    "     bias=bias,\n",
    "     task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    peft = get_peft_model(flux, lora_config)\n",
    "    \n",
    "    # model.print_trainable_parameters()\n",
    "    for name, param in peft.named_parameters():\n",
    "        if \"lora\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    return peft\n",
    "\n",
    "peft = get_flux_peft_model_to_train(pipe.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba63915-0c34-4c74-a3bd-3731754a39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(prompt=\"paul mescal\", output_type=\"latent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639be0ae-5e94-47ff-a70e-4751cfbac6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=15,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    ").images[0]\n",
    "image.save(\"flux-dev.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff4e97-ca86-4aac-8cf1-f6056ef9dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=15,\n",
    "    max_sequence_length=512,\n",
    "    generator=torch.Generator(\"cpu\").manual_seed(0),\n",
    "    output_type=\"latent\"\n",
    ").images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1a291-efd6-4288-bf92-a996411ebde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafff2e7-bd24-47a1-af71-53364ac94d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from invoke_training._shared.data.data_loaders.image_caption_sd_dataloader import build_image_caption_sd_dataloader\n",
    "from invoke_training.config.data.data_loader_config import ImageCaptionSDDataLoaderConfig\n",
    "\n",
    "\n",
    "def _build_data_loader(\n",
    "    data_loader_config: ImageCaptionSDDataLoaderConfig,\n",
    "    batch_size: int,\n",
    "    use_masks: bool = False,\n",
    "    text_encoder_output_cache_dir: Optional[str] = None,\n",
    "    vae_output_cache_dir: Optional[str] = None,\n",
    "    shuffle: bool = True,\n",
    "    sequential_batching: bool = False,\n",
    ") -> DataLoader:\n",
    "    if data_loader_config.type == \"IMAGE_CAPTION_SD_DATA_LOADER\":\n",
    "        return build_image_caption_sd_dataloader(\n",
    "            config=data_loader_config,\n",
    "            batch_size=batch_size,\n",
    "            use_masks=use_masks,\n",
    "            text_encoder_output_cache_dir=text_encoder_output_cache_dir,\n",
    "            text_encoder_cache_field_to_output_field={\"text_encoder_output\": \"text_encoder_output\"},\n",
    "            vae_output_cache_dir=vae_output_cache_dir,\n",
    "            shuffle=shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df77ea1-9cb4-489b-8b97-e34e528ff2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/training_images\"\n",
    "prompt_file=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/prompts.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c2d81-8ab3-4df3-af5c-a7925e1f353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cfd3e-8f78-461d-a37c-21430b33405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "class FluxPeftWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for FLUX model with PEFT that handles parameter name inconsistencies\n",
    "    between the main forward method and internal components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model, lora_config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Apply PEFT to the base model\n",
    "        self.model = get_peft_model(base_model, lora_config)\n",
    "        \n",
    "        # Store reference to the original model for direct access if needed\n",
    "        self.base_model = base_model\n",
    "        \n",
    "    def forward(self, hidden_states, timestep, pooled_projections, encoder_hidden_states, \n",
    "                txt_ids, img_ids, return_dict=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom forward method that handles parameter name discrepancies.\n",
    "        \"\"\"\n",
    "        # Call the model's forward method with the correct parameter names\n",
    "        output = self.model(\n",
    "            hidden_states=hidden_states, \n",
    "            timestep=timestep,\n",
    "            pooled_projections=pooled_projections,  # FLUX main forward expects plural\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            txt_ids=txt_ids,\n",
    "            img_ids=img_ids,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def train(self, mode=True):\n",
    "        self.model.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "        return self\n",
    "    \n",
    "    def requires_grad_(self, requires_grad=True):\n",
    "        self.model.requires_grad_(requires_grad)\n",
    "        return self\n",
    "    \n",
    "    def save_pretrained(self, output_dir):\n",
    "        \"\"\"Save the LoRA weights\"\"\"\n",
    "        self.model.save_pretrained(output_dir)\n",
    "    \n",
    "    def load_pretrained(self, pretrained_model_name_or_path):\n",
    "        \"\"\"Load pretrained LoRA weights\"\"\"\n",
    "        self.model.load_adapter(pretrained_model_name_or_path)\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"Return the parameters of the model\"\"\"\n",
    "        return self.model.parameters()\n",
    "    \n",
    "    def named_parameters(self):\n",
    "        \"\"\"Return the named parameters of the model\"\"\"\n",
    "        return self.model.named_parameters()\n",
    "    \n",
    "    # Add any other methods you need to delegate to the underlying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3891a2-b7a7-4356-b89e-72e34d3ce4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967aa671-8b76-4fd3-9477-0569df155384",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FluxPeftWrapper(transformer, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6342295-3bc7-4484-b56a-2b0b230cde2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.transformer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492cbb8-e3e5-4721-bfd8-502ece67497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522b536-6dd6-4cf9-ac77-968571ee73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "                        r=16,\n",
    "                        lora_alpha=32, \n",
    "                        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"proj_out\", \"proj_mlp\", \"linear\", \"linear_1\", \"linear_2\"],\n",
    "                        lora_dropout=0.05, \n",
    "                        bias=\"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf9add-285a-48a7-bdf3-34daba034383",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = pipe.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ea888-5d8d-4f1c-92aa-840452670250",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"proj_out\", \"proj_mlp\", \"linear\", \"linear_1\", \"linear_2\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    # Add this line to specify which parameters to keep in forward pass\n",
    "    modules_to_save=None  # Don't save any non-LoRA modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to transformer model\n",
    "print(\"Applying LoRA to transformer...\")\n",
    "transformer = get_peft_model(transformer, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11038847-2713-420a-aee6-ed488b5f409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import types\n",
    "from typing import List, Optional, Union, Dict, Any\n",
    "from diffusers import FluxPipeline, FlowMatchEulerDiscreteScheduler\n",
    "from accelerate import Accelerator\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "# import wandb\n",
    "\n",
    "\n",
    "# Custom dataset for text-to-image training\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, prompt_file, tokenizer, tokenizer_2, transforms):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        \n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) \n",
    "                           if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Load prompts        \n",
    "        with open(prompt_file, 'r') as f:\n",
    "            self.prompts = json.load(f)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        # Get prompt for this image\n",
    "        img_name = os.path.basename(image_path)\n",
    "        prompt = self.prompts.get(img_name, \"\")\n",
    "        \n",
    "        # CLIP tokenization\n",
    "        clip_tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # T5 tokenization\n",
    "        t5_tokens = self.tokenizer_2(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,  # Max sequence length for T5\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"images\": image,\n",
    "            \"clip_input_ids\": clip_tokens.input_ids[0],\n",
    "            \"clip_attention_mask\": clip_tokens.attention_mask[0],\n",
    "            \"t5_input_ids\": t5_tokens.input_ids[0],\n",
    "            \"t5_attention_mask\": t5_tokens.attention_mask[0],\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "\n",
    "\n",
    "# Helper function to freeze params except LoRA\n",
    "def freeze_params(model):\n",
    "    \"\"\"\n",
    "    Freezes all parameters in a PyTorch model except for LoRA parameters.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" not in name.lower():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    # Print statistics\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(f\"Frozen parameters: {total_params - trainable_params:,} ({100 * (total_params - trainable_params) / total_params:.2f}%)\")\n",
    "\n",
    "\n",
    "def parse_args(default=True):\n",
    "    if default:\n",
    "        return get_default_args()\n",
    "    parser = argparse.ArgumentParser(description=\"FLUX LoRA Training Script\")\n",
    "    parser.add_argument(\"--pretrained_model_name\", type=str, required=True, help=\"Path to pretrained FLUX model\")\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, required=True, help=\"Directory containing training images\")\n",
    "    parser.add_argument(\"--prompt_file\", type=str, required=True, help=\"File containing prompts for images\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"flux_lora_output\", help=\"Output directory for saving model\")\n",
    "    parser.add_argument(\"--resolution\", type=int, default=512, help=\"Training resolution\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=1, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--max_train_steps\", type=int, default=1000, help=\"Maximum number of training steps\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"Number of gradient accumulation steps\")\n",
    "    parser.add_argument(\"--use_8bit_adam\", action=\"store_true\", help=\"Use 8-bit Adam optimizer\")\n",
    "    parser.add_argument(\"--mixed_precision\", type=str, default=\"fp16\", choices=[\"no\", \"fp16\", \"bf16\"], help=\"Mixed precision training\")\n",
    "    parser.add_argument(\"--lora_rank\", type=int, default=16, help=\"Rank for LoRA adaptation\")\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32, help=\"Alpha parameter for LoRA\")\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05, help=\"Dropout probability for LoRA layers\")\n",
    "    parser.add_argument(\"--use_wandb\", action=\"store_true\", help=\"Use Weights & Biases for logging\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_default_args():\n",
    "    args = argparse.Namespace()\n",
    "    \n",
    "    # Model paths\n",
    "    args.pretrained_model_name = \"black-forest-labs/FLUX.1-dev\"\n",
    "    args.output_dir = \"flux_lora_output\"\n",
    "\n",
    "    # Training Data\n",
    "    args.train_data_dir=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/training_images\"\n",
    "    args.prompt_file=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/prompts.json\"\n",
    "    \n",
    "    # Training parameters\n",
    "    args.resolution = 768  # Higher resolution for better quality\n",
    "    args.train_batch_size = 1  # FLUX is memory-intensive\n",
    "    args.learning_rate = 5e-5  # Slightly lower learning rate for stability\n",
    "    args.max_train_steps = 2000  # More steps for better convergence\n",
    "    args.gradient_accumulation_steps = 4  # For effective batch size of 4\n",
    "    args.use_8bit_adam = True  # Save memory with 8-bit optimizer\n",
    "    args.mixed_precision = \"fp16\"  # Use mixed precision to save memory\n",
    "    \n",
    "    # LoRA parameters\n",
    "    args.lora_rank = 32  # Higher rank for better adaptation capacity\n",
    "    args.lora_alpha = 64  # Alpha = 2 * rank is a good rule of thumb\n",
    "    args.lora_dropout = 0.05  # Standard dropout value for LoRA\n",
    "    \n",
    "    # Miscellaneous\n",
    "    args.seed = 42  # Standard random seed\n",
    "    \n",
    "    return args\n",
    "    \n",
    "def train():\n",
    "    args = parse_args(default=True)\n",
    "    print(args)\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision\n",
    "    )\n",
    "    \n",
    "    # Load FLUX pipeline\n",
    "    print(\"Loading FLUX pipeline...\")\n",
    "    pipeline = FluxPipeline.from_pretrained(\n",
    "        args.pretrained_model_name,\n",
    "        torch_dtype=torch.float16 if args.mixed_precision == \"fp16\" else torch.float32,\n",
    "    )\n",
    "\n",
    "    # Extract components from the pipeline\n",
    "    transformer = pipeline.transformer\n",
    "    vae = pipeline.vae\n",
    "    text_encoder = pipeline.text_encoder\n",
    "    text_encoder_2 = pipeline.text_encoder_2\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    tokenizer_2 = pipeline.tokenizer_2\n",
    "    noise_scheduler = pipeline.scheduler\n",
    "    \n",
    "    # Configure LoRA for the transformer\n",
    "    target_modules = [\n",
    "        \"to_q\",  # Query projection\n",
    "        \"to_k\",  # Key projection\n",
    "        \"to_v\",  # Value projection\n",
    "        \"to_out.0\",  # Output projection\n",
    "        \"ff.net.0.proj\",  # MLP first projection\n",
    "        \"ff.net.2\",  # MLP second projection\n",
    "    ]\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to transformer model\n",
    "    print(\"Applying LoRA to transformer...\")\n",
    "    transformer = get_peft_model(transformer, lora_config)\n",
    "    \n",
    "    # Now add the custom forward method to fix the parameter name mismatch\n",
    "    print(\"Applying custom forward method...\")\n",
    "    \n",
    "    # Store original forward method for reference\n",
    "    original_forward = transformer.forward\n",
    "    \n",
    "    # Define a custom forward method that fixes the parameter name mismatch\n",
    "    def custom_forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        pooled_projections=None,\n",
    "        timestep=None,\n",
    "        img_ids=None,\n",
    "        txt_ids=None,\n",
    "        guidance=None,\n",
    "        joint_attention_kwargs=None,\n",
    "        controlnet_block_samples=None,\n",
    "        controlnet_single_block_samples=None,\n",
    "        return_dict=True,\n",
    "        controlnet_blocks_repeat=False,\n",
    "        **kwargs):\n",
    "        \"\"\"\n",
    "        Custom forward method that directly patches the parameter name mismatch in the base model.\n",
    "        \"\"\"\n",
    "        # Create a direct patch to the model's time_text_embed method\n",
    "        # This is much simpler and more reliable than wrapping the function\n",
    "        \n",
    "        # Store the original method\n",
    "        original_forward = self.base_model.forward\n",
    "        \n",
    "        # Define a direct patched version of the base model's forward\n",
    "        def patched_base_forward(**kwargs):\n",
    "            # Access the original time_text_embed method\n",
    "            original_time_text_embed = self.base_model.time_text_embed\n",
    "            \n",
    "            # Temporarily create a custom method that accepts pooled_projections\n",
    "            def renamed_time_text_embed(inner_self, timestep, *args):\n",
    "                # For the case with no guidance\n",
    "                if len(args) == 0 or (len(args) == 1 and args[0] is None):\n",
    "                    return original_time_text_embed(timestep, pooled_projections)\n",
    "                # For the case with guidance\n",
    "                elif guidance is not None:\n",
    "                    return original_time_text_embed(timestep, guidance, pooled_projections)\n",
    "                # For any other case\n",
    "                else:\n",
    "                    return original_time_text_embed(timestep, args[0], pooled_projections)\n",
    "            \n",
    "            # Store the original methods\n",
    "            original_time_text_embed_forward = self.base_model.time_text_embed.__call__\n",
    "            \n",
    "            try:\n",
    "                # Replace the method temporarily\n",
    "                self.base_model.time_text_embed.__call__ = renamed_time_text_embed\n",
    "                \n",
    "                # Call the original forward with all parameters\n",
    "                return original_forward(**kwargs)\n",
    "            finally:\n",
    "                # Restore the original method\n",
    "                self.base_model.time_text_embed.__call__ = original_time_text_embed_forward\n",
    "        \n",
    "        # Call our patched forward with the provided parameters\n",
    "        return patched_base_forward(\n",
    "            hidden_states=hidden_states,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            pooled_projections=pooled_projections,\n",
    "            timestep=timestep,\n",
    "            img_ids=img_ids,\n",
    "            txt_ids=txt_ids,\n",
    "            guidance=guidance,\n",
    "            joint_attention_kwargs=joint_attention_kwargs,\n",
    "            controlnet_block_samples=controlnet_block_samples,\n",
    "            controlnet_single_block_samples=controlnet_single_block_samples,\n",
    "            return_dict=return_dict,\n",
    "            controlnet_blocks_repeat=controlnet_blocks_repeat,\n",
    "        )    \n",
    "    # Apply the custom forward method\n",
    "    transformer.forward = types.MethodType(custom_forward, transformer)\n",
    "\n",
    "    # Freeze other components and non-LoRA parameters\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    text_encoder_2.requires_grad_(False)\n",
    "    freeze_params(transformer)\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(args.resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TextImageDataset(\n",
    "        args.train_data_dir,\n",
    "        args.prompt_file,\n",
    "        tokenizer,\n",
    "        tokenizer_2,\n",
    "        transform\n",
    "    )\n",
    "    print(\"DATASET CREATED\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    print(\"DATASET LOADER CREATED\")\n",
    "\n",
    "    # Optimizer\n",
    "    print(\"INITIALIZING OPTIMIZER\")\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            optimizer = bnb.optim.AdamW8bit(transformer.parameters(), lr=args.learning_rate)\n",
    "        except ImportError:\n",
    "            print(\"bitsandbytes not found. Using regular AdamW.\")\n",
    "            optimizer = torch.optim.AdamW(transformer.parameters(), lr=args.learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(transformer.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    # Prepare for accelerator\n",
    "    print(\"PREPARE TRANSFORMER\")\n",
    "    transformer, optimizer, dataloader = accelerator.prepare(\n",
    "        transformer, optimizer, dataloader\n",
    "    )\n",
    "    \n",
    "    # Move models to device\n",
    "    print(\"MOVE MODELS TO DEVICE\")\n",
    "    device = accelerator.device\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    text_encoder_2 = text_encoder_2.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"BEGIN INIT TRAINING \")\n",
    "    global_step = 0\n",
    "    progress_bar = tqdm(range(args.max_train_steps), desc=\"Training\")\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    text_encoder_2.eval()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"BEGIN TRAINING LOOP \")\n",
    "    while global_step < args.max_train_steps:\n",
    "        transformer.train()\n",
    "        print(f\"global step: {global_step}\")\n",
    "        for batch in dataloader:\n",
    "            with accelerator.accumulate(transformer):\n",
    "                # Get image and text inputs\n",
    "                images = batch[\"images\"].to(device)\n",
    "                clip_input_ids = batch[\"clip_input_ids\"].to(device)\n",
    "                clip_attention_mask = batch[\"clip_attention_mask\"].to(device)\n",
    "                t5_input_ids = batch[\"t5_input_ids\"].to(device)\n",
    "                t5_attention_mask = batch[\"t5_attention_mask\"].to(device)\n",
    "                \n",
    "                # Encode text inputs\n",
    "                with torch.no_grad():\n",
    "                    # Encode with CLIP\n",
    "                    clip_outputs = text_encoder(\n",
    "                        clip_input_ids,\n",
    "                        attention_mask=clip_attention_mask,\n",
    "                        output_hidden_states=False\n",
    "                    )\n",
    "                    pooled_prompt_embeds = clip_outputs.pooler_output\n",
    "                    \n",
    "                    # Encode with T5\n",
    "                    t5_outputs = text_encoder_2(\n",
    "                        t5_input_ids,\n",
    "                        attention_mask=t5_attention_mask,\n",
    "                        output_hidden_states=False\n",
    "                    )[0]\n",
    "                    prompt_embeds = t5_outputs\n",
    "                    \n",
    "                    # Prepare text IDs (consistent with FluxPipeline implementation)\n",
    "                    text_ids = torch.zeros(prompt_embeds.shape[1], 3).to(device=device)\n",
    "                    \n",
    "                    # Encode images to latent space\n",
    "                    images = images.to(dtype=torch.float16)\n",
    "                    latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "                    \n",
    "                    # Pack latents (based on FLUX pipeline implementation)\n",
    "                    batch_size, num_channels, height, width = latents.shape\n",
    "                    latents = pipeline._pack_latents(latents, batch_size, num_channels, height, width)\n",
    "                    \n",
    "                    # Prepare latent image IDs\n",
    "                    latent_image_ids = pipeline._prepare_latent_image_ids(\n",
    "                        batch_size, height // 2, width // 2, device, latents.dtype\n",
    "                    )\n",
    "                    \n",
    "                    # Add noise to latents\n",
    "                    noise = torch.randn_like(latents)\n",
    "                    timestep = torch.randint(\n",
    "                        0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device\n",
    "                    )\n",
    "                    # Use scale_noise instead of add_noise for FlowMatchEulerDiscreteScheduler\n",
    "                    noisy_latents = noise_scheduler.scale_noise(latents, timestep, noise)\n",
    "\n",
    "                # Forward through transformer\n",
    "                print(\"forward step through transformer\")\n",
    "                model_pred = transformer(\n",
    "                    hidden_states=noisy_latents,\n",
    "                    timestep=timestep / 1000.0,\n",
    "                    pooled_projections=pooled_prompt_embeds,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    txt_ids=text_ids,\n",
    "                    img_ids=latent_image_ids,\n",
    "                    return_dict=False\n",
    "                )[0]\n",
    "                \n",
    "                # Flow matching loss: predict the vector field (noise - latents)\n",
    "                # instead of just the noise as in standard diffusion\n",
    "                target = noise - latents\n",
    "                \n",
    "                # Calculate the flow matching loss\n",
    "                # We use MSE between model prediction and the target vector field\n",
    "                loss = F.mse_loss(model_pred, target)\n",
    "\n",
    "                print(\"backpropagate\")\n",
    "                # Backpropagate and optimize\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Update progress\n",
    "            if accelerator.is_main_process:\n",
    "                progress_bar.update(1)\n",
    "                if args.use_wandb:\n",
    "                    wandb.log({\"loss\": loss.item()})\n",
    "                \n",
    "                if global_step % 100 == 0:\n",
    "                    print(f\"Step {global_step}: loss = {loss.item()}\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % 500 == 0:\n",
    "                    # Unwrap the model\n",
    "                    unwrapped_transformer = accelerator.unwrap_model(transformer)\n",
    "                    \n",
    "                    # Save LoRA weights\n",
    "                    unwrapped_transformer.save_pretrained(os.path.join(args.output_dir, f\"checkpoint-{global_step}\"))\n",
    "            \n",
    "            global_step += 1\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "    \n",
    "    # Save final model\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_transformer = accelerator.unwrap_model(transformer)\n",
    "        \n",
    "        # Save LoRA weights\n",
    "        unwrapped_transformer.save_pretrained(args.output_dir)\n",
    "        print(f\"Model saved to {args.output_dir}\")\n",
    "    \n",
    "    # Close wandb\n",
    "    # if args.use_wandb:\n",
    "    #     wandb.finish()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa287bf-f469-4f5c-a6d1-6770075aa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58bcaa-6ce0-4b06-a870-3258ff1e9e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PEFT version:\", importlib.metadata.version(\"peft\"))\n",
    "print(\"Diffusers version:\", importlib.metadata.version(\"diffusers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31874440-0df0-4bc3-92eb-7edfd666bcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = vae.encode(images).latent_dist.sample() * 0.18215\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9442b-1b1b-43f1-8c6d-1b25f933d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/training_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef575d-1790-458d-9adb-ffad7e1c42f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5e92be-bce4-44da-8880-b1acc7ac4862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 03:12:01.355384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744427521.364729   16778 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744427521.369318   16778 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Fetching 23 files: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 46.51it/s]\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:00<00:00, 14.14it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 64.72it/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 15.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# Extract the transformer component\n",
    "transformer = pipe.transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bba0a46-1096-4d01-af6f-ad3472097668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of the update matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"linear\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the transformer\n",
    "transformer_lora = get_peft_model(transformer, lora_config)\n",
    "\n",
    "# Replace the transformer in the pipeline\n",
    "pipe.transformer = transformer_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151b75d5-7708-4765-9f36-b31a5c1bdfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set up optimizer (only training LoRA parameters)\n",
    "optimizer = AdamW(transformer_lora.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop (simplified example)\n",
    "def train_step(prompt, target_image):\n",
    "    # Convert target to latents\n",
    "    with torch.no_grad():\n",
    "        target_latents = pipe.vae.encode(target_image).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
    "        target_latents = pipe._pack_latents(target_latents, 1, 16, height, width)\n",
    "    \n",
    "    # Forward pass\n",
    "    pipe.text_encoder.eval()\n",
    "    pipe.vae.eval()\n",
    "    transformer_lora.train()\n",
    "    \n",
    "    # Encode prompt\n",
    "    prompt_embeds, pooled_prompt_embeds, text_ids = pipe.encode_prompt(prompt)\n",
    "    \n",
    "    # Generate random noise\n",
    "    latents, latent_image_ids = pipe.prepare_latents(1, 16, height, width, torch.float32, \"cuda\", None)\n",
    "    \n",
    "    # Timestep\n",
    "    timestep = torch.tensor([0.5], device=\"cuda\")\n",
    "    \n",
    "    # Forward pass through transformer\n",
    "    noise_pred = transformer_lora(\n",
    "        hidden_states=latents,\n",
    "        timestep=timestep,\n",
    "        pooled_projections=pooled_prompt_embeds,\n",
    "        encoder_hidden_states=prompt_embeds,\n",
    "        txt_ids=text_ids,\n",
    "        img_ids=latent_image_ids,\n",
    "    )[0]\n",
    "    \n",
    "    # Loss calculation\n",
    "    loss = F.mse_loss(noise_pred, target_latents)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be34e41-c478-4856-9b23-d246ea6daa39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1780574919.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_16778/1780574919.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "dataset = TextImageDataset(\n",
    "    train_data_dir=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/training_images\",\n",
    "    prompt_file=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/prompts.json\",\n",
    "    tokenizer=,\n",
    "    tokenizer_2,\n",
    "    transform,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4c9cb-a15f-4c3f-8549-4755cc0371ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
