{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e837c-5bca-48e9-a724-6f55f7868d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-14 01:52:40.896230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744595560.905171  149938 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744595560.909430  149938 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(pretrained_model_name='black-forest-labs/FLUX.1-dev', output_dir='flux_lora_output', train_data_dir='/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/training_images', prompt_file='/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/prompts.json', resolution=768, train_batch_size=1, learning_rate=5e-05, max_train_steps=2000, gradient_accumulation_steps=4, use_8bit_adam=True, mixed_precision='fp16', lora_rank=32, lora_alpha=64, lora_dropout=0.05, seed=42)\n",
      "Loading FLUX pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00, 19.04it/s]\n",
      "\u001b[Ading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.01it/s]\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:00<00:00,  8.79it/s]\n",
      "\u001b[Ading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\u001b[Ading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  6.69it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.91it/s]\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA to transformer...\n",
      "Total parameters: 11,957,441,600\n",
      "Trainable parameters: 56,033,280 (0.47%)\n",
      "Frozen parameters: 11,901,408,320 (99.53%)\n",
      "DATASET CREATED\n",
      "DATASET LOADER CREATED\n",
      "INITIALIZING OPTIMIZER\n",
      "bitsandbytes not found. Using regular AdamW.\n",
      "PREPARE TRANSFORMER\n",
      "MOVE MODELS TO DEVICE\n",
      "BEGIN INIT TRAINING \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING LOOP \n",
      "<class 'diffusers.models.transformers.transformer_flux.FluxTransformer2DModel'>\n",
      "<class 'peft.peft_model.PeftModel'>\n",
      "global step: 0\n",
      "timestep:  torch.Size([1])\n",
      "latents:  torch.Size([1, 2304, 64])\n",
      "noise:  torch.Size([1, 2304, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2293\u001b[0m                 )\n\u001b[1;32m   2294\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_draw_disabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2810\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   2811\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3080\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3083\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             im, l, b, trans = self.make_image(\n\u001b[0m\u001b[1;32m    647\u001b[0m                 renderer, renderer.get_image_magnification())\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    954\u001b[0m         clip = ((self.get_clip_box() or self.axes.bbox) if self.get_clip_on()\n\u001b[1;32m    955\u001b[0m                 else self.figure.bbox)\n\u001b[0;32m--> 956\u001b[0;31m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0m\u001b[1;32m    957\u001b[0m                                 magnification, unsampled=unsampled)\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    553\u001b[0m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rgb_to_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scalar_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                 output_alpha = _resample(  # resample alpha channel\n\u001b[0m\u001b[1;32m    556\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[1;32m    557\u001b[0m                 output = _resample(  # resample rgb channels\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     _image.resample(data, out, transform,\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0m_interpd_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported dtype"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_149938/2153683431.py\u001b[0m(378)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    376 \u001b[0;31m                \u001b[0;31m# model_pred shape: torch.Size([1, 2304, 64])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    377 \u001b[0;31m                \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 378 \u001b[0;31m                \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnoisy_latents\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_latents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    379 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    380 \u001b[0;31m                \u001b[0;31m# Calculate the flow matching loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "backpropagating loss: Step 0: loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/2000 [38:05<1268:56:55, 2285.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer step completed\n",
      "Step 0: loss = nan\n",
      "timestep:  torch.Size([1])\n",
      "latents:  torch.Size([1, 2304, 64])\n",
      "noise:  torch.Size([1, 2304, 64])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2293\u001b[0m                 )\n\u001b[1;32m   2294\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_draw_disabled\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2810\u001b[0;31m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   2811\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3080\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m         mimage._draw_list_compositing_images(\n\u001b[0m\u001b[1;32m   3083\u001b[0m             renderer, self, artists, self.figure.suppressComposite)\n\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             im, l, b, trans = self.make_image(\n\u001b[0m\u001b[1;32m    647\u001b[0m                 renderer, renderer.get_image_magnification())\n\u001b[1;32m    648\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    954\u001b[0m         clip = ((self.get_clip_box() or self.axes.bbox) if self.get_clip_on()\n\u001b[1;32m    955\u001b[0m                 else self.figure.bbox)\n\u001b[0;32m--> 956\u001b[0;31m         return self._make_image(self._A, bbox, transformed_bbox, clip,\n\u001b[0m\u001b[1;32m    957\u001b[0m                                 magnification, unsampled=unsampled)\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    553\u001b[0m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rgb_to_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scalar_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                 output_alpha = _resample(  # resample alpha channel\n\u001b[0m\u001b[1;32m    556\u001b[0m                     self, A[..., 3], out_shape, t, alpha=alpha)\n\u001b[1;32m    557\u001b[0m                 output = _resample(  # resample rgb channels\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mresample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     _image.resample(data, out, transform,\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0m_interpd_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported dtype"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_149938/2153683431.py\u001b[0m(378)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    376 \u001b[0;31m                \u001b[0;31m# model_pred shape: torch.Size([1, 2304, 64])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    377 \u001b[0;31m                \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 378 \u001b[0;31m                \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnoisy_latents\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_latents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    379 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    380 \u001b[0;31m                \u001b[0;31m# Calculate the flow matching loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import types\n",
    "from typing import List, Optional, Union, Dict, Any\n",
    "from diffusers import FluxPipeline, FlowMatchEulerDiscreteScheduler\n",
    "from accelerate import Accelerator\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "# import wandb\n",
    "\n",
    "\n",
    "# Custom dataset for text-to-image training\n",
    "class TextImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, prompt_file, tokenizer, tokenizer_2, transforms):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        \n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) \n",
    "                           if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Load prompts        \n",
    "        with open(prompt_file, 'r') as f:\n",
    "            self.prompts = json.load(f)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        # Get prompt for this image\n",
    "        img_name = os.path.basename(image_path)\n",
    "        prompt = self.prompts.get(img_name, \"\")\n",
    "        \n",
    "        # CLIP tokenization\n",
    "        clip_tokens = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # T5 tokenization\n",
    "        t5_tokens = self.tokenizer_2(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,  # Max sequence length for T5\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"images\": image,\n",
    "            \"clip_input_ids\": clip_tokens.input_ids[0],\n",
    "            \"clip_attention_mask\": clip_tokens.attention_mask[0],\n",
    "            \"t5_input_ids\": t5_tokens.input_ids[0],\n",
    "            \"t5_attention_mask\": t5_tokens.attention_mask[0],\n",
    "            \"prompt\": prompt\n",
    "        }\n",
    "\n",
    "\n",
    "# Helper function to freeze params except LoRA\n",
    "def freeze_params(model):\n",
    "    \"\"\"\n",
    "    Freezes all parameters in a PyTorch model except for LoRA parameters.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" not in name.lower():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    # Print statistics\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(f\"Frozen parameters: {total_params - trainable_params:,} ({100 * (total_params - trainable_params) / total_params:.2f}%)\")\n",
    "\n",
    "\n",
    "def parse_args(default=True):\n",
    "    if default:\n",
    "        return get_default_args()\n",
    "    parser = argparse.ArgumentParser(description=\"FLUX LoRA Training Script\")\n",
    "    parser.add_argument(\"--pretrained_model_name\", type=str, required=True, help=\"Path to pretrained FLUX model\")\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, required=True, help=\"Directory containing training images\")\n",
    "    parser.add_argument(\"--prompt_file\", type=str, required=True, help=\"File containing prompts for images\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"flux_lora_output\", help=\"Output directory for saving model\")\n",
    "    parser.add_argument(\"--resolution\", type=int, default=512, help=\"Training resolution\")\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=1, help=\"Batch size for training\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--max_train_steps\", type=int, default=1000, help=\"Maximum number of training steps\")\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"Number of gradient accumulation steps\")\n",
    "    parser.add_argument(\"--use_8bit_adam\", action=\"store_true\", help=\"Use 8-bit Adam optimizer\")\n",
    "    parser.add_argument(\"--mixed_precision\", type=str, default=\"fp16\", choices=[\"no\", \"fp16\", \"bf16\"], help=\"Mixed precision training\")\n",
    "    parser.add_argument(\"--lora_rank\", type=int, default=16, help=\"Rank for LoRA adaptation\")\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=32, help=\"Alpha parameter for LoRA\")\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.05, help=\"Dropout probability for LoRA layers\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_default_args():\n",
    "    args = argparse.Namespace()\n",
    "    \n",
    "    # Model paths\n",
    "    args.pretrained_model_name = \"black-forest-labs/FLUX.1-dev\"\n",
    "    args.output_dir = \"flux_lora_output\"\n",
    "\n",
    "\n",
    "    # Training Data\n",
    "    args.train_data_dir=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/training_images\"\n",
    "    args.prompt_file=\"/home/ubuntu/Sam/invoke-training/src/paul_mescal_training/prompts.json\"\n",
    "    \n",
    "    # Training parameters\n",
    "    args.resolution = 768  # Higher resolution for better quality\n",
    "    args.train_batch_size = 1  # FLUX is memory-intensive\n",
    "    args.learning_rate = 5e-5  # Slightly lower learning rate for stability\n",
    "    args.max_train_steps = 2000  # More steps for better convergence\n",
    "    args.gradient_accumulation_steps = 4  # For effective batch size of 4\n",
    "    args.use_8bit_adam = True  # Save memory with 8-bit optimizer\n",
    "    args.mixed_precision = \"fp16\"  # Use mixed precision to save memory\n",
    "    \n",
    "    # LoRA parameters\n",
    "    args.lora_rank = 32  # Higher rank for better adaptation capacity\n",
    "    args.lora_alpha = 64  # Alpha = 2 * rank is a good rule of thumb\n",
    "    args.lora_dropout = 0.05  # Standard dropout value for LoRA\n",
    "    \n",
    "    # Miscellaneous\n",
    "    args.seed = 42  # Standard random seed\n",
    "    \n",
    "    return args\n",
    "    \n",
    "def train():\n",
    "    args = parse_args(default=True)\n",
    "    print(args)\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision\n",
    "    )\n",
    "    \n",
    "    # Load FLUX pipeline\n",
    "    print(\"Loading FLUX pipeline...\")\n",
    "    pipeline = FluxPipeline.from_pretrained(\n",
    "        args.pretrained_model_name,\n",
    "        torch_dtype=torch.float16 if args.mixed_precision == \"fp16\" else torch.float32,\n",
    "    )\n",
    "\n",
    "    # Extract components from the pipeline\n",
    "    transformer = pipeline.transformer\n",
    "    vae = pipeline.vae\n",
    "    text_encoder = pipeline.text_encoder\n",
    "    text_encoder_2 = pipeline.text_encoder_2\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    tokenizer_2 = pipeline.tokenizer_2\n",
    "    noise_scheduler = pipeline.scheduler\n",
    "    \n",
    "    # Configure LoRA for the transformer\n",
    "    target_modules = [\n",
    "        \"to_q\",  # Query projection\n",
    "        \"to_k\",  # Key projection\n",
    "        \"to_v\",  # Value projection\n",
    "        \"to_out.0\",  # Output projection\n",
    "        \"ff.net.0.proj\",  # MLP first projection\n",
    "        \"ff.net.2\",  # MLP second projection\n",
    "    ]\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=args.lora_rank,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        # bias=\"none\",\n",
    "        # task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to transformer model\n",
    "    print(\"Applying LoRA to transformer...\")\n",
    "    transformer = get_peft_model(transformer, lora_config)\n",
    "\n",
    "    # Freeze other components and non-LoRA parameters\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    text_encoder_2.requires_grad_(False)\n",
    "    freeze_params(transformer)\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(args.resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TextImageDataset(\n",
    "        args.train_data_dir,\n",
    "        args.prompt_file,\n",
    "        tokenizer,\n",
    "        tokenizer_2,\n",
    "        transform\n",
    "    )\n",
    "    print(\"DATASET CREATED\")\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    print(\"DATASET LOADER CREATED\")\n",
    "\n",
    "    # Optimizer\n",
    "    print(\"INITIALIZING OPTIMIZER\")\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            optimizer = bnb.optim.AdamW8bit(transformer.parameters(), lr=args.learning_rate)\n",
    "        except ImportError:\n",
    "            print(\"bitsandbytes not found. Using regular AdamW.\")\n",
    "            optimizer = torch.optim.AdamW(transformer.parameters(), lr=args.learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(transformer.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    # Prepare for accelerator\n",
    "    print(\"PREPARE TRANSFORMER\")\n",
    "    transformer, optimizer, dataloader = accelerator.prepare(\n",
    "        transformer, optimizer, dataloader\n",
    "    )\n",
    "    \n",
    "    # Move models to device\n",
    "    print(\"MOVE MODELS TO DEVICE\")\n",
    "    device = accelerator.device\n",
    "    vae = vae.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    text_encoder_2 = text_encoder_2.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"BEGIN INIT TRAINING \")\n",
    "    global_step = 0\n",
    "    progress_bar = tqdm(range(args.max_train_steps), desc=\"Training\")\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    text_encoder_2.eval()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"BEGIN TRAINING LOOP \")\n",
    "    print(type(pipeline.transformer))\n",
    "    print(type(transformer))\n",
    "    while global_step < args.max_train_steps:\n",
    "        transformer.train()\n",
    "        print(f\"global step: {global_step}\")\n",
    "        for batch in dataloader:\n",
    "            with accelerator.accumulate(transformer):\n",
    "                # Get image and text inputs\n",
    "                images = batch[\"images\"].to(device)\n",
    "                clip_input_ids = batch[\"clip_input_ids\"].to(device)\n",
    "                clip_attention_mask = batch[\"clip_attention_mask\"].to(device)\n",
    "                t5_input_ids = batch[\"t5_input_ids\"].to(device)\n",
    "                t5_attention_mask = batch[\"t5_attention_mask\"].to(device)\n",
    "                \n",
    "                # Encode text inputs\n",
    "                with torch.no_grad():\n",
    "                    # Encode with CLIP\n",
    "                    clip_outputs = text_encoder(\n",
    "                        clip_input_ids,\n",
    "                        attention_mask=clip_attention_mask,\n",
    "                        output_hidden_states=False\n",
    "                    )\n",
    "                    pooled_prompt_embeds = clip_outputs.pooler_output\n",
    "                    \n",
    "                    # Encode with T5\n",
    "                    t5_outputs = text_encoder_2(\n",
    "                        t5_input_ids,\n",
    "                        attention_mask=t5_attention_mask,\n",
    "                        output_hidden_states=False\n",
    "                    )[0]\n",
    "                    prompt_embeds = t5_outputs\n",
    "                    \n",
    "                    # Prepare text IDs (consistent with FluxPipeline implementation)\n",
    "                    text_ids = torch.zeros(prompt_embeds.shape[1], 3).to(device=device)\n",
    "                    \n",
    "                    # Encode images to latent space\n",
    "                    images = images.to(dtype=torch.float16)\n",
    "                    latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
    "                    \n",
    "                    # Pack latents (based on FLUX pipeline implementation)\n",
    "                    batch_size, num_channels, height, width = latents.shape\n",
    "                    latents = pipeline._pack_latents(latents, batch_size, num_channels, height, width)\n",
    "                    \n",
    "                    # Prepare latent image IDs\n",
    "                    latent_image_ids = pipeline._prepare_latent_image_ids(\n",
    "                        batch_size, height // 2, width // 2, device, latents.dtype\n",
    "                    )\n",
    "                    \n",
    "                    # Add noise to latents\n",
    "                    #torch.Size([1, 2304, 64])\n",
    "                    # noise = torch.randn_like(latents)\n",
    "                    # get random timestep to train on\n",
    "                    # timestep = torch.randint(\n",
    "                    #     0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), \n",
    "                    #     device=latents.device, generator=None\n",
    "                    # ).long()\n",
    "                    # timestep = timestep.expand(latents.shape[0]).to(latents.dtype)\n",
    "\n",
    "                    # Use scale_noise instead of add_noise for FlowMatchEulerDiscreteScheduler\n",
    "                    # print(\"USE scale noise for FlowMatchEulerDiscreteScheduler\")\n",
    "                    # print(\"timestep: \", timestep)\n",
    "                    # noisy_latents = noise_scheduler.scale_noise(latents, timestep, noise)\n",
    "\n",
    "                # Forward through transformer\n",
    "                # Convert inputs to float32 for training stability\n",
    "                # noisy_latents = noisy_latents.to(dtype=torch.float32)\n",
    "                # timestep = timestep.to(dtype=torch.int64)  # timesteps should be int64/long\n",
    "                pooled_prompt_embeds = pooled_prompt_embeds.to(dtype=torch.float32)\n",
    "                prompt_embeds = prompt_embeds.to(dtype=torch.float32)\n",
    "                text_ids = text_ids.to(dtype=torch.float32)\n",
    "                latent_image_ids = latent_image_ids.to(dtype=torch.float32)\n",
    "                guidance_scale = 1.0\n",
    "                guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n",
    "                guidance = guidance.expand(latents.shape[0])\n",
    "\n",
    "                \n",
    "                #TODO: pass num_inference_steps, sigmas, mu\n",
    "                #WE are only doing 1 step per run not prepring for full diffusion \n",
    "                # timesteps, num_inference_steps, num_warmup_steps = prepare_timesteps(latents.shape[1], \n",
    "                # noise_scheduler,\n",
    "                # device,\n",
    "                # num_inference_steps=1000,\n",
    "                # sigmas=None,\n",
    "                # mu=None\n",
    "                # )\n",
    "                noisy_latents, prev_latents, noise,timestep = get_noised_latent_at_random_timestep(sample_latents=latents, scheduler=noise_scheduler)\n",
    "                timestep = timestep.to(dtype=torch.int64)  # timesteps should be int64/long\n",
    "                if True:\n",
    "                    print(\"DECODING AND DISPLAYING\")\n",
    "                    decode_and_display_latent(noisy_latents, prev_latents, noise, vae)\n",
    "                \n",
    "                model_pred = transformer(\n",
    "                    hidden_states=noisy_latents,\n",
    "                    timestep=timestep,\n",
    "                    pooled_projections=pooled_prompt_embeds,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    guidance=guidance,\n",
    "                    txt_ids=text_ids,\n",
    "                    img_ids=latent_image_ids,\n",
    "                    return_dict=False\n",
    "                )[0]\n",
    "                # latents = noise_scheduler.step(model_pred, timestep, latents, return_dict=False)\n",
    "\n",
    "                # Ensure target is also float32\n",
    "                # target shape: torch.Size([1, 2304, 64])\n",
    "                # model_pred shape: torch.Size([1, 2304, 64])\n",
    "                breakpoint()\n",
    "                target = (noisy_latents - prev_latents).to(dtype=torch.float32)\n",
    "                \n",
    "                # Calculate the flow matching loss\n",
    "                loss = F.mse_loss(model_pred, target)\n",
    "\n",
    "                print(f\"backpropagating loss: Step {global_step}: loss = {loss.item()}\")\n",
    "                if (loss.item() < 0.11 and global_step > 24) or global_step == 24:\n",
    "                    print(type(pipeline.transformer))\n",
    "                    print(type(transformer))                    \n",
    "\n",
    "                # Backpropagate and optimize\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                print(f\"optimizer step completed\")\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # Update progress\n",
    "            if accelerator.is_main_process:\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                if global_step % 100 == 0:\n",
    "                    print(f\"Step {global_step}: loss = {loss.item()}\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % 500 == 0:\n",
    "                    # Unwrap the model\n",
    "                    unwrapped_transformer = accelerator.unwrap_model(transformer)\n",
    "                    \n",
    "                    # Save LoRA weights\n",
    "                    unwrapped_transformer.save_pretrained(os.path.join(args.output_dir, f\"checkpoint-{global_step}\"))\n",
    "                    # display_image(pipeline)\n",
    "            \n",
    "            global_step += 1\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "    \n",
    "    # Save final model\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_transformer = accelerator.unwrap_model(transformer)\n",
    "        \n",
    "        # Save LoRA weights\n",
    "        unwrapped_transformer.save_pretrained(args.output_dir)\n",
    "        print(f\"Model saved to {args.output_dir}\")\n",
    "    \n",
    "    # Close wandb\n",
    "def forward_for_training(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    timestep,\n",
    "    pooled_projections,\n",
    "    encoder_hidden_states,\n",
    "    guidance,\n",
    "    txt_ids,\n",
    "    img_ids,\n",
    "    sigmas = None,\n",
    "    mu = None,\n",
    "    num_inference_steps = None,\n",
    "    num_warmup_steps = None,\n",
    "    _num_timesteps = None,\n",
    "):\n",
    "    pass\n",
    "def get_random_timestep(noise_scheduler):\n",
    "    \"\"\"\n",
    "    Sample a random timestep for diffusion model training.\n",
    "    \n",
    "    During diffusion model training, each example only processes one random timestep\n",
    "    per training iteration (not the full sequence). This function samples that timestep\n",
    "    based on the noise scheduler's configuration.\n",
    "    \"\"\"\n",
    "    return np.randint(\n",
    "        0, noise_scheduler.config.num_train_timesteps\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_timesteps(image_seq_len, scheduler, device, num_inference_steps=1000, sigmas=None, mu=None):\n",
    "    # 5. Prepare timesteps\n",
    "    from diffusers.pipelines.flux.pipeline_flux import calculate_shift, retrieve_timesteps\n",
    "    sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n",
    "\n",
    "    mu = calculate_shift(\n",
    "        image_seq_len,\n",
    "        scheduler.config.get(\"base_image_seq_len\", 256),\n",
    "        scheduler.config.get(\"max_image_seq_len\", 4096),\n",
    "        scheduler.config.get(\"base_shift\", 0.5),\n",
    "        scheduler.config.get(\"max_shift\", 1.15),\n",
    "    )\n",
    "    timesteps, num_inference_steps = retrieve_timesteps(\n",
    "        scheduler,\n",
    "        num_inference_steps,\n",
    "        device,\n",
    "        sigmas=sigmas,\n",
    "        mu=mu,\n",
    "    )\n",
    "\n",
    "    num_warmup_steps = max(len(timesteps) - num_inference_steps * scheduler.order, 0)\n",
    "    return timesteps, num_inference_steps, num_warmup_steps\n",
    "\n",
    "def decode_and_display_latent(cur_latent, prev_latent, noise, vae):\n",
    "    # Unpack latents using FLUX pipeline's method\n",
    "    batch_size, num_patches, channels = cur_latent.shape\n",
    "    \n",
    "    # VAE applies 8x compression on images but we must also account for packing which requires\n",
    "    # latent height and width to be divisible by 2\n",
    "    height = int(np.sqrt(num_patches)) * 2  # Multiply by 2 since we packed 2x2 patches\n",
    "    width = height\n",
    "    \n",
    "    # Unpack current latents\n",
    "    cur_latent = cur_latent.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n",
    "    cur_latent = cur_latent.permute(0, 3, 1, 4, 2, 5)\n",
    "    cur_latent = cur_latent.reshape(batch_size, channels // 4, height, width)\n",
    "    \n",
    "    # Unpack previous latents\n",
    "    prev_latent = prev_latent.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n",
    "    prev_latent = prev_latent.permute(0, 3, 1, 4, 2, 5)\n",
    "    prev_latent = prev_latent.reshape(batch_size, channels // 4, height, width)\n",
    "\n",
    "    # Scale the latents back to VAE range\n",
    "    cur_latent_scaled = (cur_latent / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "    prev_latent_scaled = (prev_latent / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "    \n",
    "    # Decode latents to images\n",
    "    with torch.no_grad():\n",
    "        cur_image = vae.decode(cur_latent_scaled).sample\n",
    "        prev_image = vae.decode(prev_latent_scaled).sample\n",
    "    \n",
    "    # Convert to display format\n",
    "    cur_image = (cur_image / 2 + 0.5).clamp(0, 1)\n",
    "    prev_image = (prev_image / 2 + 0.5).clamp(0, 1)\n",
    "    \n",
    "    # Display images\n",
    "    print(cur_image.shape)\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Convert to numpy and ensure proper format for display\n",
    "    cur_img_np = cur_image[0].permute(1, 2, 0).cpu().numpy()\n",
    "    prev_img_np = prev_image[0].permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # For noise visualization, take mean across channels to get 2D representation\n",
    "    noise_np = noise[0].reshape(height, width, -1).cpu().numpy()\n",
    "    noise_np = noise_np.mean(axis=-1)  # Take mean across channels\n",
    "    noise_np = (noise_np - noise_np.min()) / (noise_np.max() - noise_np.min())  # Normalize\n",
    "    \n",
    "    ax1.imshow(cur_img_np)\n",
    "    ax1.set_title('Current Timestep')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(prev_img_np)\n",
    "    ax2.set_title('Previous Timestep')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3.imshow(noise_np, cmap='gray')  # Use grayscale colormap for noise\n",
    "    ax3.set_title('Noise')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()  # Clean up\n",
    "    \n",
    "\n",
    "\n",
    "def get_noised_latent_at_random_timestep(sample_latents, scheduler, generator=None):\n",
    "    \"\"\"\n",
    "    Get a noised latent at a random timestep along with its previous timestep value.\n",
    "    \n",
    "    During diffusion model training, we need to sample a random timestep for each example.\n",
    "    This function noises the latent to that timestep and also calculates what the latent\n",
    "    would be at the previous timestep, which is useful for loss calculation.\n",
    "    \n",
    "    Args:\n",
    "        latent: The original clean latent tensor (shape [B, C, H, W])\n",
    "        scheduler: An instance of FlowMatchEulerDiscreteScheduler\n",
    "        generator: Optional random number generator for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (cur_latent, prev_latent, timestep)\n",
    "            - cur_latent: The noised latent at the random timestep\n",
    "            - prev_latent: The previous timestep latent (needed for loss calculation)\n",
    "            - timestep: The randomly selected timestep\n",
    "    \"\"\"\n",
    "    # Sample a random timestep\n",
    "    num_train_timesteps = scheduler.config.num_train_timesteps\n",
    "    timestep = torch.randint(\n",
    "        0, num_train_timesteps, (sample_latents.shape[0],), \n",
    "        device=sample_latents.device, generator=generator\n",
    "    ).long()\n",
    "    timestep = torch.clamp(timestep, min=1)\n",
    "    # Convert to float timestep as expected by the scheduler\n",
    "    # float_timestep = timestep.float() / num_train_timesteps * scheduler.config.num_train_timesteps\n",
    "    \n",
    "    # Generate random noise\n",
    "    noise = torch.randn_like(sample_latents)\n",
    "    \n",
    "    # Get the noised latent at the current timestep\n",
    "    print(\"timestep: \", timestep.shape)\n",
    "    print(\"latents: \", sample_latents.shape)\n",
    "    print(\"noise: \", noise.shape)\n",
    "\n",
    "    # cur_latents = scheduler.scale_noise(sample=latents, timestep=float_timestep, noise=noise)\n",
    "    cur_latents = scheduler.scale_noise(\n",
    "        sample=sample_latents,\n",
    "        timestep=timestep,\n",
    "        noise=noise\n",
    "    )\n",
    "    \n",
    "    # Calculate what the previous timestep would be\n",
    "    # For the last timestep, we use 0 (no noise)\n",
    "    prev_timestep = torch.clamp(timestep - 1, min=0)\n",
    "    # Get the noised latent at the previous timestep\n",
    "    prev_latents = scheduler.scale_noise(\n",
    "        sample=sample_latents,\n",
    "        timestep=prev_timestep,\n",
    "        noise=noise\n",
    "    )        \n",
    "    return cur_latents, prev_latents, noise, timestep\n",
    "\n",
    "\n",
    "def display_image(pipeline):\n",
    "    prompt = \"Paul Mescal holding a sign that says hello world\"\n",
    "        \n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            image = pipeline(\n",
    "                prompt,\n",
    "                height=1024,\n",
    "                width=1024,\n",
    "                guidance_scale=3.5,\n",
    "                num_inference_steps=5,\n",
    "                max_sequence_length=512,\n",
    "                generator=torch.Generator(\"cpu\").manual_seed(0)\n",
    "            ).images[0]\n",
    "            \n",
    "            # Save image with timestamp\n",
    "            import time\n",
    "            timestamp = int(time.time())\n",
    "            save_path = f\"generated_image_{timestamp}.png\"\n",
    "            image.save(save_path)\n",
    "            print(f\"Saved generated image to {save_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error during image generation:\")\n",
    "        print(e)\n",
    "        print(\"Failed to generate image\")\n",
    "\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5cfc7a-8c3c-4f3c-8af0-e814e96c5f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
